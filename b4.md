## B.4. Mạng và các yếu tố ảnh hưởng đến streaming

Trong truyền dòng video qua mạng IP, chất lượng trải nghiệm (QoE) phụ thuộc trực tiếp vào các đặc trưng của đường truyền:

- **Độ trễ (latency)**  
- **Jitter (biến thiên độ trễ)**  
- **Packet loss (mất gói)**  
- **Throughput và băng thông hiệu dụng**

Các yếu tố này không độc lập: \( \text{latency} \) và \( \text{packet loss} \) quyết định **throughput TCP**, từ đó ảnh hưởng tới khả năng chọn bitrate, trạng thái buffer và hiện tượng stall. Trong phần này, ta xây dựng các mô hình toán cơ bản cho từng yếu tố và liên hệ với bài toán video streaming.

---

### B.4.1. Độ trễ (latency)

#### B.4.1.1. Thành phần của độ trễ

Độ trễ một chiều từ client đến server (hoặc ngược lại) tại thời điểm \( t \) có thể viết:

$$
D(t) = d_{\text{prop}} + d_{\text{trans}}(t) + d_{\text{queue}}(t) + d_{\text{proc}}(t)
$$

trong đó:

- \( d_{\text{prop}} \): **propagation delay** – tỉ lệ với khoảng cách vật lý và loại môi trường truyền.  
- \( d_{\text{trans}}(t) \): **transmission delay** – thời gian để “đẩy” toàn bộ gói lên đường truyền.  
- \( d_{\text{queue}}(t) \): **queuing delay** – thời gian chờ trong hàng đợi tại router/switch.  
- \( d_{\text{proc}}(t) \): **processing delay** – xử lý tại NIC, kernel, stack TCP/IP, ứng dụng.

Với một gói có kích thước \( L \) (bit), băng thông đường truyền vật lý \( C \) (bit/s), ta có:

$$
d_{\text{trans}} = \frac{L}{C}
$$

Trong nhiều mạng backbone, \( d_{\text{prop}} \) và \( d_{\text{proc}} \) tương đối ổn định, còn \( d_{\text{queue}}(t) \) biến thiên mạnh theo mức độ tắc nghẽn.

Độ trễ hai chiều (RTT) đo được bởi TCP/QUIC:

$$
\text{RTT}(t) \approx D_{\text{client}\to\text{server}}(t) + D_{\text{server}\to\text{client}}(t)
$$

RTT là tham số then chốt trong **congestion control** và **ước lượng throughput**.

#### B.4.1.2. Độ trễ end-to-end trong video streaming

Với dịch vụ **VoD trên HTTP**, độ trễ người dùng quan tâm nhất là **startup delay**:

$$
T_{\text{startup}} 
\approx T_{\text{DNS}} + T_{\text{TCP/TLS}} + T_{\text{HTTP}} + T_{\text{buf}}
$$

trong đó:

- \( T_{\text{DNS}} \): phân giải DNS / chọn edge CDN;  
- \( T_{\text{TCP/TLS}} \): thiết lập kết nối TCP/QUIC + handshake TLS;  
- \( T_{\text{HTTP}} \): request manifest/playlist, request segment đầu;  
- \( T_{\text{buf}} \): thời gian nạp đủ \( B_{\text{start}} \) giây video vào buffer trước khi bắt đầu play.

Nếu ta xét riêng phần **tải video**, với throughput trung bình \( C_{\text{eff}} \) (bit/s), bitrate representation chọn là \( R \) (bit/s), buffer mục tiêu khi start là \( B_{\text{start}} \) (giây video), thì thời gian để nạp đủ phần video tương ứng:

$$
T_{\text{buf}} \approx \frac{R \cdot B_{\text{start}}}{C_{\text{eff}}}
$$

Ví dụ: \( R = 3\,\text{Mbit/s} \), \( B_{\text{start}} = 4\,\text{s} \), \( C_{\text{eff}} = 12\,\text{Mbit/s} \)  
→ \( T_{\text{buf}} \approx 1\,\text{s} \). Nếu throughput giảm còn \( 4\,\text{Mbit/s} \) thì \( T_{\text{buf}} \approx 3\,\text{s} \).

Với **live streaming** (HLS/DASH live, LL-HLS, WebRTC), độ trễ end-to-end:

$$
T_{\text{E2E}}
\approx T_{\text{capt}} + T_{\text{enc}} + T_{\text{pack}} + T_{\text{dist}} + T_{\text{net}} + T_{\text{client-buf}}
$$

- \( T_{\text{capt}} \): capture và xử lý khung tại camera.  
- \( T_{\text{enc}} \): mã hóa (codec delay, look-ahead, cấu trúc GOP).  
- \( T_{\text{pack}} \): đóng gói segment hoặc frame (chunk).  
- \( T_{\text{dist}} \): phân phối qua CDN hoặc server trung gian.  
- \( T_{\text{net}} \): tổng latency mạng (nhiều hops).  
- \( T_{\text{client-buf}} \): buffer phát phía client để chống jitter.

Trong HTTP live streaming “truyền thống”, \( T_{\text{E2E}} \) thường bằng **vài lần** độ dài segment (ví dụ 3–4 segment × 2 s = 6–8 s), trong khi các giải pháp **low-latency** cố gắng giảm \( T_{\text{E2E}} \) xuống còn 1–3 s, hoặc **sub-second** với WebRTC.

---

### B.4.2. Jitter (biến thiên độ trễ)

#### B.4.2.1. Định nghĩa thống kê

Gọi \( d_i \) là độ trễ (one-way) của gói thứ \( i \) hoặc chênh lệch thời gian giữa **arrival** và **thời điểm hiển thị dự kiến**. Jitter thường được mô hình như **độ lệch chuẩn** hoặc **mean absolute deviation** của \( d_i \):

- Trung bình:

  $$
  \bar{d} = \frac{1}{N} \sum_{i=1}^N d_i
  $$

- Độ lệch chuẩn:

  $$
  \sigma_d = \sqrt{\frac{1}{N-1} \sum_{i=1}^N (d_i - \bar{d})^2}
  $$

- Mean absolute deviation:

  $$
  J_{\text{MAD}} = \frac{1}{N} \sum_{i=1}^N |d_i - \bar{d}|
  $$

Trong thực tế, RFC 3550 định nghĩa **interarrival jitter** cho RTP dùng một bộ lọc mũ bậc nhất.

#### B.4.2.2. Jitter theo RFC 3550 cho RTP

RFC 3550 định nghĩa jitter dựa trên sự khác biệt giữa khoảng cách gói tại sender và tại receiver \[B23]. Ký hiệu:

- \( S_i \): RTP timestamp của gói \( i \) (đơn vị clock của media).  
- \( R_i \): thời điểm đến của gói \( i \) tại receiver, chuyển đổi sang **cùng đơn vị** với \( S_i \).  

Khoảng cách tương đối (relative transit time) giữa hai gói \( i \) và \( j \):

$$
D(i,j) = (R_j - R_i) - (S_j - S_i)
      = (R_j - S_j) - (R_i - S_i)
$$

Interarrival jitter \( J(i) \) được cập nhật liên tục với mỗi gói đến:

$$
J(i) = J(i-1) + \frac{\left| D(i-1,i) \right| - J(i-1)}{16}
$$

- \( J(i) \): jitter hiện tại ở gói thứ \( i \);  
- hệ số \( \frac{1}{16} \) là **gain** của bộ lọc mũ, tạo cân bằng giữa tốc độ hội tụ và khả năng khử nhiễu.

Giá trị \( J(i) \) (đơn vị timestamp) được encoder lưu trong trường **interarrival jitter** của RTCP report để các bên khác có thể đánh giá mức độ biến động trễ. Trong streaming real-time, jitter lớn buộc client phải:

- tăng \( T_{\text{client-buf}} \) (buffer playout);  
- hoặc chấp nhận **late loss**: gói đến quá trễ so với deadline hiển thị → bị bỏ.

Giả sử ta chọn **playout deadline** là:

$$
t_{\text{play},i} = S_i / f_{\text{clk}} + D_{\text{target}}
$$

trong đó \( f_{\text{clk}} \) là clock rate, \( D_{\text{target}} \) là **playout delay** cố định. Một gói bị xem là đến muộn nếu:

$$
R_i > t_{\text{play},i}
$$

Khi jitter \( \sigma_d \) lớn, để giữ xác suất đến muộn \( P(R_i > t_{\text{play},i}) \le \varepsilon \), ta cần chọn \( D_{\text{target}} \) đủ lớn, ví dụ:

$$
D_{\text{target}} \approx \mu_d + k \sigma_d
$$

với \( \mu_d \) là độ trễ trung bình, \( k \) phụ thuộc ngưỡng \( \varepsilon \) (VD \( k\approx 2 \) cho \( \varepsilon \approx 2.5\% \)). Điều này giải thích trade-off:

- tăng buffer để chống jitter → tăng **latency**;  
- giảm buffer để giảm latency → dễ mất frame do đến muộn.

#### B.4.2.3. Jitter và QoE

Jitter cao dẫn đến:

- **giật hình** (video bị freeze ngắn do chờ đầy buffer lại);  
- **méo hình** trong RTP/UDP nếu không kịp phục hồi (FEC/ARQ);  
- tăng thời gian **stall** trong HTTP streaming khi throughput dao động khiến tốc độ tải segment không ổn định.

Trong mô hình QoE, jitter góp phần:

- tăng thời gian stall (do buffer tụt nhanh khi một vài segment tải chậm đột biến);  
- tăng biến thiên bitrate (thuật toán ABR phản ứng nhầm với những spike ngắn).

Các thuật toán ABR hiện đại thường dùng **trung bình trượt, harmonic mean, hoặc mô hình ML** để lọc nhiễu jitter trong throughput ước lượng.

---

### B.4.3. Packet loss (mất gói)

#### B.4.3.1. Định nghĩa và đo lường

Trong RTP, RFC 3550 định nghĩa \[B23]:

- \( N_{\text{exp}} \): số gói **expected** (dựa trên sequence number).  
- \( N_{\text{rcv}} \): số gói thực tế nhận được.  
- Cumulative loss:

  $$
  N_{\text{loss}} = N_{\text{exp}} - N_{\text{rcv}}
  $$

- Packet loss fraction trong một khoảng thời gian giữa hai report:

  $$
  p = \frac{\Delta N_{\text{loss}}}{\Delta N_{\text{exp}}}
  $$

Trong các tool đo mạng, \( p \) thường được đo dưới dạng phần trăm mất gói trên số gói gửi trong một khoảng thời gian (ICMP, UDP probe, RTP,…).

Nguồn gây mất gói:

- **Congestion**: hàng đợi đầy, router drop;  
- **Lỗi đường truyền**: nhiễu trên wireless, cáp lỗi;  
- **Routing/transient failure**;  
- **Late loss**: gói đến muộn sau deadline và bị bỏ.

#### B.4.3.2. Ảnh hưởng tới video streaming

**Trên RTP/UDP (real-time streaming, WebRTC):**

- Mất gói ở layer **video** → mất macroblock, mất frame, hoặc drop cả frame tùy theo kỹ thuật concealment.  
- Mất gói ở **I-frame** gây ảnh hưởng rộng hơn so với P/B-frame do các frame sau phụ thuộc.  
- Một phần có thể được bù nhờ:

  - **FEC (Forward Error Correction)**: gửi dữ liệu dự phòng để khôi phục gói mất;  
  - **ARQ (retransmission)**: yêu cầu gửi lại (nhưng bị giới hạn bởi yêu cầu low-latency).

**Trên HTTP/TCP (DASH/HLS):**

- Packet loss được TCP che giấu bằng **retransmission**, nhưng làm:

  - tăng RTT hiệu dụng;  
  - giảm cửa sổ congestion (cwnd) → giảm throughput;  
  - kéo dài thời gian tải segment → buffer dễ tụt.

Do đó, ngay cả khi ứng dụng không “thấy” mất gói (do TCP đảm bảo), QoE vẫn bị suy giảm thông qua **throughput thấp hơn và latency cao hơn**.

#### B.4.3.3. Packet loss và throughput TCP (Mathis model)

Một mô hình kinh điển cho throughput tối đa của một luồng TCP trong trạng thái ổn định (congestion avoidance) là công thức **Mathis**:

$$
\text{Rate} \lesssim \frac{\text{MSS}}{\text{RTT}} \cdot \frac{1}{\sqrt{p}}
$$

trong đó:

- \( \text{Rate} \): throughput dữ liệu (bit/s hoặc byte/s);  
- \( \text{MSS} \): Maximum Segment Size (byte);  
- \( \text{RTT} \): round-trip time trung bình (s);  
- \( p \): xác suất mất gói (0 < \( p \) < 1).

Ý nghĩa:

- Khi \( p \) tăng **gấp 4 lần**, throughput tối đa giảm khoảng **một nửa** (vì \( 1/\sqrt{p} \)).  
- Khi \( \text{RTT} \) tăng, throughput giảm tuyến tính.  

Ví dụ: cùng \( \text{MSS} \), nếu \( \text{RTT} \) tăng từ 50 ms lên 200 ms (gấp 4) và \( p \) giữ nguyên, throughput giảm **4 lần**. Điều này đặc biệt nghiêm trọng cho streaming qua các mạng có RTT lớn (liên lục địa, vệ tinh).

Kết luận: **packet loss nhỏ nhưng không bằng 0** cũng đủ để làm throughput TCP giảm đáng kể, hạn chế bitrate video có thể chọn, tăng nguy cơ stall.

---

### B.4.4. Ước lượng throughput và băng thông hiệu dụng

Trong adaptive streaming (MPEG-DASH, HLS), client cần ước lượng **băng thông khả dụng** để quyết định bitrate tiếp theo. Nếu ước lượng sai:

- **quá lạc quan** → chọn bitrate cao → tải segment không kịp → stall;  
- **quá bi quan** → chọn bitrate thấp → chất lượng kém dù mạng đủ tốt.

#### B.4.4.1. Ước lượng throughput từ tải segment (HTTP)

Giả sử segment thứ \( k \):

- có kích thước \( S_k \) (bit);  
- bắt đầu tải tại thời điểm \( t_k^{\text{start}} \), kết thúc tại \( t_k^{\text{end}} \);  
- thời gian tải:

  $$
  D_k = t_k^{\text{end}} - t_k^{\text{start}}
  $$

Throughput **đo được** (sample) cho segment:

$$
C_k = \frac{S_k}{D_k}
$$

Client duy trì một ước lượng trơn \( \hat{C}_k \) dựa trên các sample lịch sử. Ví dụ với **trung bình trượt mũ (EMA)**:

$$
\hat{C}_k = (1 - \alpha)\hat{C}_{k-1} + \alpha C_k,\quad 0 < \alpha \le 1
$$

- \( \alpha \) càng lớn → phản ứng nhanh nhưng nhiễu (bị jitter throughput).  
- \( \alpha \) càng nhỏ → ổn định nhưng chậm thích nghi.

Một số thuật toán dùng **harmonic mean** để giảm ảnh hưởng outlier cao:

$$
\hat{C}_k^{\text{harm}} 
= \left( \frac{1}{n} \sum_{i=k-n+1}^{k} \frac{1}{C_i} \right)^{-1}
$$

vì harmonic mean “phạt” mạnh các giá trị nhỏ, phản ánh tình huống “bottleneck” rõ hơn.

#### B.4.4.2. Băng thông hiệu dụng và overhead

Băng thông vật lý của link là \( C_{\text{link}} \) (bit/s), nhưng **băng thông hiệu dụng cho payload video** thấp hơn do:

- overhead header (IP/UDP/TCP/QUIC, RTP, TLS);  
- retransmission (TCP, ARQ);  
- FEC;  
- cross-traffic (các flow khác chia sẻ link).

Nếu \( \eta_{\text{hdr}} \) là phần trăm overhead header, \( \eta_{\text{re}} \) là phần trăm overhead do retransmission/FEC, thì băng thông hiệu dụng có thể xấp xỉ:

$$
C_{\text{eff}} \approx C_{\text{link}} \cdot (1 - \eta_{\text{hdr}} - \eta_{\text{re}}) \cdot (1 - \rho_{\text{cross}})
$$

trong đó:

- \( \rho_{\text{cross}} \) là phần băng thông bị chiếm bởi các flow khác.

Trong các hệ thống ABR, **client không biết trực tiếp \( C_{\text{link}} \) hay \( \rho_{\text{cross}} \)**; nó chỉ “thấy” \( C_k \), vốn đã bao gồm mọi overhead. Vì vậy các model throughput phải được thiết kế sao cho **không nhầm lẫn spike ngắn với xu hướng dài hạn**.

#### B.4.4.3. Throughput estimation nâng cao

Các nghiên cứu gần đây đề xuất:

- **Model dự báo throughput theo thời gian**: dùng chuỗi lịch sử \( \{C_{k-m}, \dots, C_k\} \), RTT, jitter để dự đoán phân phối throughput tương lai \( C_{k+1}, \dots, C_{k+h} \).  
- **MPC-based ABR**: kết hợp dự báo throughput và mô hình buffer để giải bài toán tối ưu (theo QoE) trong cửa sổ tương lai (detailed hơn ở mục C.2.6.5).  
- **Học sâu (Deep Learning)**: sử dụng mạng RNN, LSTM, attention để khai thác pattern phức tạp trong trace mạng, có thể cho dự báo đa mốc thời gian, dự đoán cả **độ bất định** để điều chỉnh chiến lược bitrate.

Trong các mô hình này, ước lượng throughput không chỉ là một giá trị điểm \( \hat{C}_k \), mà là **phân phối** hoặc **dải giá trị**; ABR sẽ chọn bitrate sao cho:

- xác suất **stall** thấp;  
- chất lượng trung bình cao;  
- biến thiên bitrate mượt.

#### B.4.4.4. Kết nối throughput – buffer – QoE

Kết hợp với mô hình buffer rời rạc ở B.2.3, throughput ước lượng \( \hat{C}_k \) và bitrate chọn \( R_k \) ảnh hưởng tới **diễn tiến buffer**:

$$
B_{k+1} 
= \max\{0,\ B_k - D_k\} + T_{\text{seg}},\quad
D_k = \frac{S_k}{C_k} = \frac{R_k T_{\text{seg}}}{C_k}
$$

- Nếu \( C_k \approx \hat{C}_k \) và \( R_k \ll C_k \): \( D_k < T_{\text{seg}} \), buffer tăng → ít stall.  
- Nếu \( R_k \approx C_k \) và có jitter mạnh → một số \( C_k \) thấp đột ngột, \( D_k > T_{\text{seg}} \), buffer tụt và có thể chạm 0.  

Trong các hàm QoE dạng:

$$
\text{QoE} = \sum_{k} u(Q_k) - \alpha \sum_{k} |Q_{k+1} - Q_k| - \beta T_{\text{stall,total}}
$$

throughput và các yếu tố mạng (\(\text{RTT}\), jitter, loss) tác động vào:

- \( Q_k \) (chất lượng/bitrate đạt được);  
- \( T_{\text{stall,total}} \) (tổng thời gian stall);  
- độ mượt của chuỗi \( Q_k \).

Do đó, mọi nghiên cứu về thuật toán ABR và cơ chế streaming đều phải **gắn liền** với phân tích mạng ở mức \( \text{latency} \)–\( \text{jitter} \)–\( \text{loss} \)–\( \text{throughput} \), chứ không thể xem các yếu tố này tách rời.

---

### Tài liệu tham khảo cho mục B.4 (định dạng IEEE)

\[B23] H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobson, “RTP: A Transport Protocol for Real-Time Applications,” RFC 3550, IETF, Jul. 2003.  

\[B24] M. Mathis, J. Semke, J. Mahdavi, and T. Ott, “The macroscopic behavior of the TCP congestion avoidance algorithm,” *ACM SIGCOMM Comput. Commun. Rev.*, vol. 27, no. 3, pp. 67–82, Jul. 1997.  

\[B25] G. P. Fettweis *et al.*, “Impacts of delay and packet loss on TCP throughput,” in *Proc. IEEE ICC Workshops*, 2011.  

\[B26] G. T. Nguyen, A. Sathiaseelan, and J. Crowcroft, “Cost-aware adaptive bitrate video streaming,” in *Proc. IFIP Networking*, 2018.  

\[B27] T.-Y. Huang *et al.*, “A buffer-based approach to rate adaptation: Evidence from a large video streaming service,” in *Proc. ACM SIGCOMM*, 2014, pp. 187–198.  

\[B28] X. Yin, V. Sekar, and B. Sinopoli, “Toward a principled framework to design dynamic adaptive streaming algorithms over HTTP,” in *Proc. ACM HotNets*, 2014.  

\[B29] S. Akhshabi, A. C. Begen, and C. Dovrolis, “An experimental evaluation of rate-adaptation algorithms in adaptive streaming over HTTP,” in *Proc. ACM MMSys*, 2011.  

\[B30] Z. Li, G. Simon, and A. Ksentini, “QoE-aware rate adaptation for DASH-based HTTP video streaming,” in *Proc. IEEE ICC*, 2013.
