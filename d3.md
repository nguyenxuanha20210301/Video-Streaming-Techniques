## D.3. Mô hình QoE trong video streaming

**Quality of Experience (QoE)** trong video streaming được hiểu là cảm nhận chủ quan của người dùng cuối về chất lượng dịch vụ, thường được chuẩn hóa trên thang Mean Opinion Score (MOS) từ 1 đến 5 (Bad–Excellent) trong các khuyến nghị ITU-T/ITU-R \[D16]. QoE không chỉ phụ thuộc vào **chất lượng hiển thị video** (artifact nén, độ phân giải, frame rate) mà còn chịu ảnh hưởng mạnh từ:

- $T_{\text{init}}$: độ trễ khởi tạo (initial startup delay).  
- $T_{\text{stall}}$: tổng thời gian stall/rebuffer.  
- $N_{\text{stall}}$: số lần stall.  
- $Q_k$: chất lượng từng đoạn (bitrate, resolution, VMAF/PSNR/SSIM).  
- $\Delta Q_k$: mức độ dao động chất lượng (bitrate/quality switching).

Trong phần này ta tập trung vào ba trụ cột:

1. **ITU-T P.1203** – mô hình QoE tham số hoá (parametric) cho HTTP adaptive streaming.  
2. **Netflix VMAF** – metric đánh giá chất lượng full-reference dựa trên học máy.  
3. So sánh **PSNR – SSIM – VMAF** dưới góc nhìn lý thuyết và thực nghiệm.

---

### D.3.1. ITU-T P.1203 – mô hình đánh giá QoE end-to-end

#### D.3.1.1. Tổng quan và kiến trúc mô hình

**ITU-T Recommendation P.1203** là mô hình chuẩn hoá đầu tiên của ITU-T dành riêng cho việc **dự đoán QoE của dịch vụ HTTP Adaptive Streaming (HAS)** trên transport tin cậy (TCP) \[D17–D19]. P.1203 là một bộ khuyến nghị gồm:

- **P.1203.1** – module **video ngắn hạn** ($P_v$):  
  Tính điểm chất lượng video mỗi 1 giây: $q_v(t) \in [1,5]$ (MOS).

- **P.1203.2** – module **audio & audiovisual** ($P_a$):  
  Tính chất lượng audio $q_a(t)$ và kết hợp với video thành $q_{av}(t)$.

- **P.1203.3** – module **quality integration** ($P_q$):  
  Tích hợp các chuỗi $q_{av}(t)$ với thông tin stall, delay, switching để ra **MOS phiên**:
  $$
  MOS_{\text{P1203}} \in [1,5].
  $$

Các module này hoạt động **parametric**: chỉ dựa vào:

- thông tin **bitstream/metadata** (codec, resolution, frame rate, bit-rate, quantization, GOP…);  
- **chuỗi bitrate/representation** theo thời gian;  
- **profile stalling** (vị trí, số lần, tổng thời lượng stall);  
- $T_{\text{init}}$ và một số tham số playback.

Không cần video gốc (full-reference), vì vậy P.1203 rất phù hợp cho **monitoring QoE tại mạng/tập server/CDN**.

#### D.3.1.2. Module video ngắn hạn $P_v$ và audio $P_a$

Ký hiệu:

- $t$: index thời gian (thường là từng 1 giây, $t = 1,\dots,T$).  
- $q_v(t)$: điểm chất lượng video tại giây $t$ (thang MOS 1–5).  
- $q_a(t)$: điểm chất lượng audio tại giây $t$.  
- $q_{av}(t)$: điểm chất lượng **audiovisual** tích hợp tại giây $t$.

Module $P_v$ lấy các đặc trưng video tại giây $t$:

- codec (H.264/AVC, HEVC…),  
- resolution hiệu dụng $W_t \times H_t$,  
- frame rate $f_t$,  
- bitrate hiệu dụng $R_t$,  
- đặc trưng nén (ví dụ: QP trung bình, số macroblock bị skip, v.v.),

và gộp chúng vào một hàm parametric:

$$
q_v(t) 
= f_v\big(R_t, W_t, H_t, f_t, \text{codec}_t, \text{QP}_t, \ldots \big),
$$

trong đó $f_v(\cdot)$ là tổ hợp các biểu thức tuyến tính/phi tuyến (piecewise, log, power) được hiệu chỉnh bằng dữ liệu chủ quan \[D17–D19].

Tương tự, module $P_a$:

$$
q_a(t) 
= f_a\big(R^{(a)}_t, \text{codec}^{(a)}_t, \text{samplingRate}_t, \ldots\big),
$$

với $R^{(a)}_t$ bitrate audio, $\text{codec}^{(a)}_t$ (AAC, HE-AAC, Opus,…).

Module audiovisual (thuộc P.1203.2) kết hợp audio–video:

$$
q_{av}(t) = f_{av}(q_v(t), q_a(t), \text{offset}_t),
$$

trong đó $\text{offset}_t$ có thể phản ánh **A/V sync** (audio–video lip sync).

#### D.3.1.3. Module tích hợp $P_q$: từ quality chuỗi đến MOS phiên

**Quality integration module** $P_q$ gom chuỗi $q_{av}(t)$ thành một **MOS tổng thể** cho phiên xem, gắn thêm ảnh hưởng của:

- $T_{\text{init}}$: startup delay;  
- $N_{\text{stall}}$, $T_{\text{stall,total}}$;  
- vị trí các stall (đầu video, giữa, cuối);  
- mức độ dao động chất lượng (quality switching).

Ký hiệu:

- $\bar{q}_{av}$: trung bình chất lượng qua thời gian:

  $$
  \bar{q}_{av} 
  = \frac{1}{T} \sum_{t=1}^{T} q_{av}(t).
  $$

- $\Phi_{\text{stall}}$: hàm penalty do stall:

  $$
  \Phi_{\text{stall}} = g_{\text{stall}}\big(T_{\text{stall,total}}, N_{\text{stall}}, \{t_{\text{stall},i}\}\big),
  $$

  trong đó $t_{\text{stall},i}$ là vị trí thời gian xảy ra stall thứ $i$ (đầu, giữa hay gần cuối).

- $\Phi_{\text{var}}$: penalty do dao động chất lượng:

  $$
  \Phi_{\text{var}} = \sum_{t=1}^{T-1} \left| q_{av}(t+1) - q_{av}(t) \right|. 
  $$

Một dạng tổng quát cho MOS phiên mà nhiều mô hình parametric sử dụng (minh hoạ, không trùng hẳn form chuẩn):

$$
MOS_{\text{P1203}} = \alpha_v \bar{q}_{av} - \beta_{\text{stall}} \Phi_{\text{stall}} - \beta_{\text{var}} \Phi_{\text{var}} - \beta_{\text{init}} T_{\text{init}},
$$

với:

- $\alpha_v$, $\beta_{\text{stall}}$, $\beta_{\text{var}}$, $\beta_{\text{init}}$: trọng số được hiệu chỉnh bằng test chủ quan.  
- Toàn bộ biểu thức sau đó được **cắt và ánh xạ** về khoảng MOS $[1,5]$.

Trong P.1203 thực, $P_q$ dùng các hàm phi tuyến, weight theo **recency** (các phần cuối video được “nhớ” nhiều hơn) và penalty đặc biệt cho stall gần đầu video (ảnh hưởng lớn tới ấn tượng đầu tiên).

#### D.3.1.4. Phạm vi và giới hạn của P.1203

**Ưu điểm:**

- Chuẩn hoá ITU-T, được thiết kế và validate trên >1000 sequence với >25 000 rating chủ quan, áp dụng cho **HTTP adaptive streaming** (DASH/HLS) \[D17–D19].  
- Không cần reference video, chỉ cần bitstream/metadata + log sự kiện (stall, bitrate, delay).  
- Có thể tích hợp vào **probe** tại mạng, set-top-box, QoE monitoring server.

**Giới hạn:**

- Phạm vi gốc: phiên xem 30 s–5 phút (sau đó có mở rộng), chưa phải cho các phiên cực dài hoặc các nội dung rất đặc thù (VR, 8K, game streaming).  
- Mô hình parametric được “fit” cho pattern HAS điển hình; khi áp dụng cho loại traffic khác (ví dụ ultra-low-latency, UDP-based) cần cân nhắc.  
- Cần **chuỗi thông tin chi tiết** (timeline bitrate/resolution/stall) – nếu không log đủ thì khó áp dụng đúng.

Trong bối cảnh project, P.1203 là “điểm tựa” để:

- đánh giá QoE của các thuật toán ABR khác nhau (BOLA/MPC/Pensieve) trên cùng trace;  
- phân tích ảnh hưởng của stall và switching tới MOS.

---

### D.3.2. Netflix VMAF – Video Multi-Method Assessment Fusion

#### D.3.2.1. Nguyên lý và pipeline chung

**Video Multi-Method Assessment Fusion (VMAF)** là một **full-reference video quality metric** do Netflix phát triển, kết hợp các đặc trưng video và mô hình học máy để dự đoán **điểm chất lượng chủ quan** trên thang khoảng $[0,100]$ \[D20–D23]. VMAF được thiết kế nhằm:

- tương quan tốt hơn với đánh giá chủ quan so với PSNR/SSIM truyền thống đối với nội dung streaming đa dạng (phim, series, hoạt hình, v.v.);  
- hỗ trợ tối ưu hoá ladder mã hoá (bitrate–resolution) cho dịch vụ OTT.

Pipeline cơ bản cho một cặp video:

- Video gốc (reference) $V_{\text{ref}}$;  
- Video bị suy giảm (distorted) $V_{\text{dist}}$ (sau nén, truyền, v.v.).

Các bước:

1. **Đồng bộ hoá & căn chỉnh** hai chuỗi frame (same resolution, frame rate).  
2. Với mỗi frame $n$ hoặc mỗi patch, trích xuất **vector đặc trưng**:

   - các feature multi-scale về độ nét, chi tiết, tương phản (ví dụ họ metric VIF, DLM, ADM…);  
   - thông tin về cấu trúc (tương tự SSIM/MS-SSIM nhưng được biến đổi/ngẫu hợp).

   Ký hiệu vector đặc trưng:

   $$
   \mathbf{x}_n \in \mathbb{R}^d.
   $$

3. Dùng một **mô hình hồi quy/học máy** $g(\cdot; \boldsymbol{\theta})$ (SVR, random forest, hay ensemble) để dự đoán chất lượng frame:

   $$
   \hat{q}_n = g(\mathbf{x}_n; \boldsymbol{\theta}).
   $$

4. Ghép các điểm frame-level thành **điểm sequence-level**:

   - Mặc định: trung bình cộng (mean pooling):

     $$
     VMAF = \frac{1}{N} \sum_{n=1}^{N} \hat{q}_n.
     $$

   - Một số nghiên cứu đề xuất các **temporal pooling** phức tạp hơn (max, percentile, recency-weighted…) nhằm phản ánh tốt hơn nhận thức con người.

Kết quả $VMAF$ thường:

- $0$ – rất tệ, không xem được;  
- $20$–$40$ – kém;  
- $60$–$80$ – khá/ tốt;  
- $>90$ – rất tốt, gần như không phân biệt với gốc trong bối cảnh streaming phổ thông.

VMAF đã được **open-source** (libvmaf, vmaf-dev-kit) và được cộng đồng sử dụng rộng rãi để đánh giá codec (H.264/HEVC/VP9/AV1/VVC).

#### D.3.2.2. Mô hình toán khái quát của VMAF

Tách rõ ba lớp:

1. **Feature extraction**:  
   Với mỗi frame $n$:

   $$
   \mathbf{x}_n = \big[ f_1(V_{\text{ref}}, V_{\text{dist}}), \dots, f_d(V_{\text{ref}}, V_{\text{dist}}) \big]_n,
   $$

   trong đó mỗi $f_i(\cdot)$ là một metric con (như detail loss, contrast, SSIM-like, v.v.).

2. **Regression to quality**:  
   Một bộ dữ liệu huấn luyện:

   $$
   \big\{ (\mathbf{x}_n^{(j)}, y_n^{(j)}) \big\}
   $$

   với $y_n^{(j)}$ là **MOS hoặc DMOS** đo được từ thử nghiệm chủ quan. Bài toán:

   $$
   \min_{\boldsymbol{\theta}}
   \sum_{j,n}
   \ell
   \Big(
     g(\mathbf{x}_n^{(j)}; \boldsymbol{\theta}),
     y_n^{(j)}
   \Big),
   $$

   với $\ell(\cdot)$ là hàm loss (thường là squared error với một số biến đổi logistic). Sau khi học xong $\boldsymbol{\theta}$, ta có hàm:

   $$
   \hat{q}_n = g(\mathbf{x}_n; \boldsymbol{\theta}).
   $$

3. **Temporal pooling**:

   $$ 
   VMAF = h(\hat{q}_1, \dots, \hat{q}_N),
   $$

   trong đó $h(\cdot)$ thường là mean, nhưng có thể thay bằng các hàm pooling khác để tăng tương quan với QoE.

Các phiên bản VMAF khác nhau (vmaf\_0.6.1, vmaf\_neg, 4K, phone model, v.v.) tương ứng với **bộ feature** và **mô hình hồi quy** khác nhau, được huấn luyện trên các bộ dữ liệu và điều kiện hiển thị khác nhau.

#### D.3.2.3. Ứng dụng trong tối ưu hoá streaming

Trong thực tế, VMAF được sử dụng để:

1. **Thiết kế bitrate ladder**:  
   Với một nội dung, thử nhiều cấu hình mã hoá $(R_i, W_i, H_i)$, tính $VMAF_i$; chọn ladder sao cho:

   - tại mỗi mức băng thông target $C^\*$, có representation với $R_i \le C^\*$;  
   - $VMAF_i$ tối đa hoá “VMAF per bitrate” (tối ưu hiệu năng nén).

2. **Đánh giá codec / encoder**:  
   So sánh (H.264 vs HEVC vs AV1) trên cùng bit-rate, đo VMAF trung bình:

   - codec nào cho VMAF cao hơn tại cùng $R$ → hiệu quả hơn;  
   - hoặc đạt cùng VMAF ở bitrate thấp hơn.

3. **Nghiên cứu QoE**:  
   Nhiều paper sử dụng VMAF làm proxy cho $Q_k$ trong mô hình QoE:

   $$
   u(Q_k) \propto VMAF_k,
   $$

   thay vì dùng bitrate $R_k$ trực tiếp, vì VMAF có tương quan tốt hơn với MOS.

**Hạn chế chính**:

- Tính toán nặng hơn PSNR/SSIM (cần trích xuất nhiều feature + regression).  
- Là metric full-reference: cần lưu hoặc có access video gốc; không áp dụng trực tiếp cho monitoring “trên đường” nếu không có reference.  
- Bị “overfit” một phần cho loại nội dung/dataset dùng huấn luyện; với nội dung rất khác (VR, hoạt hình extreme, UI text) có thể cần retrain hoặc thận trọng diễn giải.

---

### D.3.3. So sánh PSNR, SSIM, VMAF

Để đánh giá chất lượng video, ba metric full-reference thông dụng nhất là:

- **PSNR** (Peak Signal-to-Noise Ratio) – đơn giản, dựa trên sai số năng lượng.  
- **SSIM** (Structural Similarity) – dựa trên cấu trúc.  
- **VMAF** – fusion nhiều metric với ML, định hướng perception.

#### D.3.3.1. PSNR – Peak Signal-to-Noise Ratio

Cho một frame ảnh gốc $I$ và ảnh méo $K$ kích thước $M \times N$:

- Sai số bình phương trung bình (MSE):

  $$
  MSE = \frac{1}{MN} \sum_{i=0}^{M-1} \sum_{j=0}^{N-1} \big( I(i,j) - K(i,j) \big)^2.
  $$

- Nếu $MAX_I$ là giá trị pixel lớn nhất (ví dụ 255 với 8-bit), **PSNR** (dB):

  $$
  PSNR = 10 \log_{10} \left( \frac{MAX_I^2}{MSE} \right).
  $$

Cho video, PSNR thường được:

- tính trên kênh độ sáng (Y) cho từng frame $\Rightarrow PSNR_n$;  
- sau đó lấy trung bình:

  $$
  PSNR_{\text{avg}} = \frac{1}{N} \sum_{n=1}^N PSNR_n.
  $$

**Ưu điểm**:

- Cực kỳ dễ tính, nhanh, được dùng rộng rãi trong nghiên cứu nén video.  

**Nhược điểm**:

- Không phản ánh tốt perception: **tăng PSNR không phải lúc nào cũng “đẹp” hơn** tương ứng.  
- Không phân biệt tốt các artifact mang tính cấu trúc (blocking, ringing, blur) vốn rất nhạy với mắt người.

#### D.3.3.2. SSIM – Structural Similarity Index

SSIM của Wang *et al.* \[D21] giả thiết rằng hệ thị giác con người thích nghi tốt với **cấu trúc** hơn là sai số điểm ảnh đơn lẻ. Với hai patch $x$ và $y$:

- $\mu_x$, $\mu_y$: giá trị trung bình;  
- $\sigma_x^2$, $\sigma_y^2$: phương sai;  
- $\sigma_{xy}$: hiệp phương sai;  
- $C_1$, $C_2$: hằng số nhỏ để ổn định.

**SSIM** được định nghĩa:

$$
SSIM(x,y) = \frac{ (2\mu_x \mu_y + C_1) (2\sigma_{xy} + C_2) }{ (\mu_x^2 + \mu_y^2 + C_1) (\sigma_x^2 + \sigma_y^2 + C_2) }.$$

Trong đó:

- Term **luminance**: $\frac{2\mu_x \mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1}$;  
- Term **contrast**: $\frac{2\sigma_x \sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2}$;  
- Term **structure**: $\frac{\sigma_{xy} + C_3}{\sigma_x \sigma_y + C_3}$ (trong form mở rộng).

Giá trị:

- $SSIM \in [-1,1]$, thường trong thực tế $[0,1]$, càng gần 1 càng giống.  

Với video, người ta tính SSIM trên từng frame, rồi trung bình (hoặc dùng MS-SSIM – multi-scale SSIM).

**Ưu điểm**:

- Bám sát perception hơn PSNR, đặc biệt với các artifact cấu trúc.  

**Nhược điểm**:

- Vẫn là metric “hand-crafted”, chưa modeling đầy đủ phức tạp hệ thị giác;  
- Không trực tiếp encode các yếu tố thời gian (temporal masking, motion, v.v.).

#### D.3.3.3. Đối chiếu PSNR – SSIM – VMAF

Ta có cái nhìn tổng quan qua bảng:

```text
+---------+----------------------+---------------------------+-----------------------------+
| Metric  | Loại                | Đặc trưng chính          | Ưu / Nhược                 |
+---------+----------------------+---------------------------+-----------------------------+
| PSNR    | Full-ref, pixel-wise| Sai số bình phương (MSE) | + Rất nhanh, đơn giản      |
|         |                      |                           | - Không perceptual         |
+---------+----------------------+---------------------------+-----------------------------+
| SSIM    | Full-ref, patch-wise| Cấu trúc, độ sáng,       | + Phù hợp perception hơn   |
|         |                      | tương phản               | - Đơn mô-đun, chưa tối ưu  |
+---------+----------------------+---------------------------+-----------------------------+
| VMAF    | Full-ref, learned    | Fusion nhiều feature +   | + Tương quan MOS cao       |
|         |                      | ML, frame + pooling      | + Tối ưu cho streaming     |
|         |                      |                           | - Tính toán nặng, cần ref  |
+---------+----------------------+---------------------------+-----------------------------+
```

**Về tương quan với QoE**:

* Các benchmark gần đây chỉ ra rằng VMAF có **correlation (PLCC/SROCC)** với MOS cao hơn đáng kể so với PSNR, và nói chung cao hơn SSIM trên các tập nội dung streaming điển hình.
* Tuy nhiên, trong một số dataset đặc thù hoặc với xử lý phi tuyến (enhancement đặc biệt), PSNR/SSIM vẫn có thể cạnh tranh hoặc thậm chí vượt VMAF, cho thấy **không metric nào là “vạn năng”**.

**Về sử dụng trong nghiên cứu ABR và QoE**:

* **PSNR/SSIM** thường dùng làm baseline hoặc feature bổ sung, vì:

  * dễ tích hợp vào pipeline mô phỏng;
  * nhanh, phù hợp khi cần tính trên rất nhiều phương án encode.

* **VMAF** ngày càng được sử dụng như “thước đo chất lượng khách quan” trong:

  * thiết kế bitrate ladder,
  * đánh giá hiệu quả thuật toán ABR (chất lượng trung bình, fairness),
  * làm thành phần trong hàm utility $u(Q_k)$ của bài toán tối ưu QoE.

Trong khuôn khổ project, ta có thể:

* Sử dụng **VMAF** để đo $Q_k$ cho từng segment,
* Sử dụng **P.1203** để chuyển chuỗi $(Q_k, T_{\text{stall}}, T_{\text{init}})$ thành $MOS$ dự đoán,
* So sánh các thuật toán ABR không chỉ trên “bitrate trung bình” mà trên **QoE dự đoán gần perception**.

---

### Tài liệu tham khảo cho mục D.3 (định dạng IEEE)

[D16] ITU-R, *BT.500-15: Methodologies for the Subjective Assessment of the Quality of Television Images*, 2023.

[D17] ITU-T, *P.1203: Parametric Bitstream-based Quality Assessment of Progressive Download and Adaptive Audiovisual Streaming Services over Reliable Transport*, ITU-T Rec. P.1203, 2017 (and subsequent amendments).

[D18] ITU-T, *P.1203.1: Bitstream-based Quality Assessment of Progressive Download and Adaptive Audiovisual Streaming Services over Reliable Transport – Video Quality Estimation Module*, 2017.

[D19] ITU-T, *P.1203.3: Parametric Bitstream-based Quality Assessment of Progressive Download and Adaptive Audiovisual Streaming Services over Reliable Transport – Quality Integration Module*, 2019.

[D20] Netflix, “VMAF – Video Multi-method Assessment Fusion,” GitHub repository, 2016–2024.

[D21] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: From error visibility to structural similarity,” *IEEE Trans. Image Process.*, vol. 13, no. 4, pp. 600–612, Apr. 2004.

[D22] F. Netflix, C.-C. J. Kuo *et al.*, “VMAF: The Journey Continues,” Netflix/USC Tech. Rep., 2018.

[D23] T. Li, M. Barkowsky, and P. Le Callet, “Improved temporal pooling for perceptual video quality assessment using VMAF,” in *Proc. Electronic Imaging*, 2020.

[D24] W. Robitza *et al.*, “HTTP adaptive streaming QoE estimation with ITU-T Rec. P.1203: Open databases and software,” in *Proc. QoMEX*, 2018.

[D25] N. Staelens *et al.*, “Assessing the impact of stalling on QoE of HTTP adaptive streaming,” in *Proc. IFIP/IEEE IM*, 2013.

[D26] O. A. Niamut *et al.*, “An overview of video quality metrics for streaming applications,” in *Proc. IEEE ICC Workshops*, 2021.

