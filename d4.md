## D.4. Congestion Control

Trong video streaming qua Internet, **congestion control (CC)** quyết định:

- tốc độ gửi dữ liệu $R_{\text{send}}$ (bit/s),  
- độ đầy hàng đợi (queue occupancy) và từ đó  
- $RTT$, $jitter$, tỷ lệ mất gói, và cuối cùng là **QoE**.

Ở lớp vận chuyển, TCP sử dụng các thuật toán CC như **Reno/NewReno**, **CUBIC**, **BBR**; trong real-time streaming (WebRTC trên RTP/UDP), ta dùng **Google Congestion Control (GCC)**. Mục này:

- mô hình hóa **AIMD TCP CC** và hai biến thể hiện đại: **CUBIC** và **BBR**;  
- phân tích **GCC** trong WebRTC, liên hệ với yêu cầu latency thấp.

---

### D.4.1. Congestion control ở tầng TCP (Cubic, BBR)

#### D.4.1.1. Mô hình AIMD TCP cơ bản

Truyền thống, TCP sử dụng **Additive Increase – Multiplicative Decrease (AIMD)** \[RFC 5681]. Ký hiệu:

- $W_n$: cửa sổ nghẽn (congestion window, đơn vị: segment) tại vòng lặp thứ $n$;  
- $MSS$: kích thước segment tối đa (byte);  
- $T_{\text{RTT}}$: round-trip time (giây);  
- $p$: xác suất mất gói (steady-state, i.i.d. mỗi gói – giả thiết đơn giản hóa);  
- $R_{\text{send}}$: tốc độ gửi (throughput), xấp xỉ:

  $$
  R_{\text{send}} \approx \frac{W_n \cdot MSS}{T_{\text{RTT}}}.
  $$

**AIMD TCP (Reno/NewReno)**:

- Mỗi RTT không có mất gói:

  $$
  W_{n+1} = W_n + 1.
  $$

- Khi phát hiện mất gói (duyệt qua 3 dupack hoặc timeout), giảm multiplicative:

  $$
  W_{n+1} = \beta W_n, \quad 0 < \beta < 1.
  $$

Với Reno, $\beta = 1/2$.

Mô hình Padhye et al. (1998) cho throughput TCP Reno \[D27]:

- Trong steady-state (packet loss do queue tràn, không vì lỗi kênh), throughput trung bình:

  $$
  R_{\text{reno}} \approx \frac{MSS}{T_{\text{RTT}}} \cdot \frac{1}{\sqrt{\frac{2}{3}p}} = \Theta\left(\frac{1}{T_{\text{RTT}}\sqrt{p}}\right).
  $$

Điều này cho thấy:

- Khi $p$ nhỏ, throughput giảm theo $\frac{1}{\sqrt{p}}$;  
- Khi $T_{\text{RTT}}$ lớn, throughput giảm tỷ lệ nghịch với $T_{\text{RTT}}$.

Vấn đề:

- Trên mạng **băng thông lớn – delay lớn (LFN)**, để đạt utilization cao, $W_n$ phải rất lớn; nhưng với AIMD tuyến tính (tăng $+1$ mỗi RTT) → hội tụ rất chậm.  
- AIMD Reno **tương đối RTT-unfair**: flow với RTT nhỏ tăng cửa sổ nhanh hơn, chiếm nhiều băng thông hơn.

CUBIC được đề xuất để giải quyết hạn chế này.

---

#### D.4.1.2. TCP CUBIC

**CUBIC** là thuật toán CC mặc định của Linux, Windows, macOS hiện nay, chuẩn hóa trong RFC 8312 và draft rfc8312bis \[D28, D29]. Ý tưởng:

- vẫn là **loss-based**, nhưng thay vì tăng **tuyến tính theo RTT** như Reno, CUBIC dùng **hàm bậc ba theo thời gian** để tăng $W(t)$;  
- mục tiêu: **RTT-fairness** tốt hơn và **scalability** cao trên đường truyền nhanh–xa.

Ký hiệu:

- $W(t)$: congestion window (segments) tại thời điểm $t$ (giây) kể từ khi nhận signal mất gói gần nhất;  
- $W_{\max}$: giá trị $W(t)$ ngay trước khi mất gói (cửa sổ cực đại trước loss);  
- $\beta$: hệ số giảm cửa sổ khi mất gói (thường $\beta \approx 0.7$);  
- $C$: hệ số tỉ lệ (thường $C \approx 0.4$);  
- $K$: tham số dịch chuyển thời gian (sec).

Sau khi bị mất gói, CUBIC đặt lại **thời gian gốc** $t = 0$, giảm cửa sổ:

$$
W(0) = (1 - \beta) W_{\max},
$$

và diễn tiến $W(t)$ theo:

$$
W(t) = C (t - K)^3 + W_{\max}.
$$

Trong đó $K$ được chọn sao cho tại thời điểm $t=0$, $W(0)$ đúng bằng $(1-\beta)W_{\max}$:

$$
(1 - \beta) W_{\max} = C(-K)^3 + W_{\max} \quad\Rightarrow\quad C K^3 = W_{\max}(1-\beta).
$$

Do đó:

$$
K = \sqrt[3]{\frac{W_{\max}(1-\beta)}{C}}.
$$

**Diễn giải hình học:**

- Khi $t < K$, $t-K$ âm, $W(t)$ nằm **dưới** $W_{\max}$, với đường cong **concave** (độ tăng lớn lúc đầu rồi giảm dần) → giảm khả năng overshoot queue.  
- Khi $t \approx K$, $W(t) \approx W_{\max}$ (vùng “plateau”);  
- Khi $t > K$, $t-K$ dương, $W(t)$ **vượt** $W_{\max}$ với profile **convex** → giúp khám phá băng thông nếu vẫn còn dư.

CUBIC thực tế sử dụng:

- một **hàm cubic chính** $W_{\text{cubic}}(t)$ như trên;  
- thêm một nhánh **TCP-friendly** để đảm bảo không vượt quá fairness với Reno trên các đường truyền băng thông thấp–trung bình: window Reno ước lượng $W_{\text{reno}}(t)$, cửa sổ thực:

  $$
  W(t) = \max\{ W_{\text{cubic}}(t),\ W_{\text{reno}}(t) \}.
  $$

**Đặc điểm chính:**

- Trong vùng **đường dài–băng rộng**, $W_{\text{cubic}}$ chiếm ưu thế → nhanh đạt mức cửa sổ phù hợp;  
- Tăng cửa sổ **ít phụ thuộc RTT** hơn Reno, giúp cải thiện RTT fairness giữa flows;  
- Triển khai rộng rãi và đã trở thành chuẩn de-facto trong Internet hiện đại.

Trong ngữ cảnh **video streaming trên HTTP/TCP**, đa số truyền tải ABR/DASH/HLS ngày nay đều chạy trên **CUBIC** (hoặc biến thể), nghĩa là hành vi **bursting, queue build-up, delay** của các flow video chịu ảnh hưởng trực tiếp bởi hàm $W(t)$ nêu trên.

---

#### D.4.1.3. BBR – Bottleneck Bandwidth and RTT

**BBR (Bottleneck Bandwidth and Round-trip propagation time)** là một họ thuật toán CC mới do Google đề xuất \[D30, D31]. Khác với Reno/CUBIC (loss-based), BBR:

- xây dựng **mô hình tường minh về đường truyền** dựa trên hai tham số:

  - $BtlBw$: ước lượng băng thông nghẽn cổ chai (bottleneck bandwidth);  
  - $RTT_{\text{prop}}$: ước lượng round-trip propagation time (RTT tối thiểu – không có queueing).

- điều khiển cả:

  - **pacing rate** (tốc độ phát gói) $R_{\text{pace}}$;  
  - **cwnd tối đa** (số bytes in-flight cho phép) $W_{\max}$;

  sao cho **lượng dữ liệu trong mạng** xấp xỉ:

  $$
  \text{inflight} \approx BtlBw \times RTT_{\text{prop}}.
  $$

##### (a) Ước lượng mô hình đường truyền

Trong BBR v1:

- **Băng thông cổ chai** $BtlBw$ ước lượng bằng **delivery rate** lớn nhất quan sát được trong một cửa sổ thời gian (moving max filter):

  - Với mỗi RTT, đo $R_{\text{deliv}} = \frac{\text{bytes delivered}}{\Delta t}$;  
  - $BtlBw$ là max của $R_{\text{deliv}}$ trong vài RTT gần nhất.

- **RTT propagation** $RTT_{\text{prop}}$ ước lượng bằng **RTT nhỏ nhất quan sát được** trong một khoảng thời gian dài hơn (vài giây):

  $$
  RTT_{\text{prop}} = \min_{\tau \in \text{window}} RTT(\tau).
  $$

Giả thiết:

- Khi hàng đợi trống, $RTT \approx RTT_{\text{prop}}$;  
- Khi queue build-up, $RTT > RTT_{\text{prop}}$.

Do đó, kết hợp $BtlBw$ và $RTT_{\text{prop}}$ cho ta **một mô hình BDP (bandwidth–delay product)**:

$$
BDP = BtlBw \cdot RTT_{\text{prop}}.
$$

##### (b) Điều khiển pacing rate và cwnd

BBR chọn:

- **Pacing rate**:

  $$
  R_{\text{pace}} = G_{\text{bw}} \times BtlBw,
  $$

  trong đó $G_{\text{bw}}$ là **gain** (hệ số) phụ thuộc phase của BBR (STARTUP, DRAIN, PROBE\_BW, PROBE\_RTT).

- **Cwnd**:

  $$
  cwnd = G_{\text{cwnd}} \times BtlBw \times RTT_{\text{prop}},
  $$

  với $G_{\text{cwnd}} \approx 2$ (cho phép inflight $\approx 2 \times BDP$ để che jitter nhỏ và burst).

Mục tiêu:

- giữ inflight gần BDP → **tránh làm đầy queue** (hạn chế bufferbloat);  
- không phụ thuộc trực tiếp vào loss như Reno/CUBIC; loss chỉ là tín hiệu bổ sung.

##### (c) Các phase hoạt động

BBR v1 có 4 phase chính \[D30]:

1. **STARTUP**:

   - tăng gain $G_{\text{bw}} > 1$ (ví dụ 2) để **double** pacing rate mỗi RTT, tìm kiếm gần đúng $BtlBw$;  
   - kết thúc khi **delivery rate** không còn tăng.

2. **DRAIN**:

   - giảm gain $G_{\text{bw}} < 1$ (ví dụ 0.7) trong một thời gian ngắn để **xả bớt hàng đợi**, đưa inflight ≈ BDP.

3. **PROBE\_BW**:

   - phase steady-state, BBR tuần hoàn qua các gain pattern để **probe băng thông thêm**:

     - vài RTT với $G_{\text{bw}}>1$ (tăng rate) để xem BtlBw có tăng không;  
     - vài RTT với $G_{\text{bw}}<1$ để xả queue;  
     - phần lớn thời gian với $G_{\text{bw}}=1$.

4. **PROBE\_RTT**:

   - định kỳ giảm inflight xuống rất thấp (vài MSS) để **đo lại $RTT_{\text{prop}}$** (RTT khi queue trống);  
   - phase này ngắn, nhưng gây drop throughput tạm thời.

##### (d) Ý nghĩa đối với video streaming

Với BBR:

- Hàng đợi thường **không bị lấp đầy** như với loss-based CC → $RTT$ và $jitter$ nhỏ hơn, độ trễ end-to-end từ server video đến CDN/edge/UE giảm;  
- Tuy nhiên, trong cạnh tranh với CUBIC, BBR có thể **aggressive hơn** trong một số kịch bản, dẫn tới unfairness;  
- Các version mới (BBR v2, v3) thêm cơ chế fairness và loss sensitivity.

Đối với **HTTP adaptive video**:

- Khi server hoặc mạng trung gian sử dụng BBR, **trace throughput $C_k$** feed vào ABR sẽ khác đáng kể so với CUBIC: ít “sawtooth loss-based” hơn, nhưng có các chu kỳ probe đặc trưng.  
- Việc mô phỏng phải lưu ý phân biệt **TCPCUBIC** vs **TCPBBR** khi đánh giá ABR.

---

### D.4.2. Congestion control trong WebRTC (GCC)

#### D.4.2.1. Mô hình delay-based của GCC

Cho real-time video (WebRTC), dùng TCP là không phù hợp (head-of-line blocking, latency cao). WebRTC sử dụng RTP/SRTP trên UDP, với **Google Congestion Control (GCC)** – một CC **delay-based + loss-based** được thiết kế đặc biệt để:

- giữ **queue nhỏ** (low-delay),  
- tránh **congestion collapse**,  
- và đạt throughput chấp nhận được \[D32, D33].

**Kiến trúc GCC** (phiên bản “trendline” trong Chrome):

1. **Bộ ước lượng delay** (delay-based estimator):

   - đo **one-way delay** (hoặc approximation) dựa trên timestamp send/recv;  
   - lọc và tính **slope (trendline)** để phát hiện queue build-up.

2. **Bộ quyết định trạng thái** (overuse / underuse / normal):

   - nếu trendline nghiêng lên (delay tăng có hệ thống) → “overuse”;  
   - nếu trendline nghiêng xuống hoặc phẳng → “underuse” / “normal”.

3. **Bộ điều khiển rate** (AIMD-like):

   - tăng tuyến tính khi underuse;  
   - giảm multiplicative khi overuse.

Ký hiệu:

- $d_k$: delay (hoặc **delay variation**) đo được cho cụm gói thứ $k$;  
- $\tilde{d}_k$: delay smoothed qua bộ lọc low-pass:

  $$
  \tilde{d}_k = (1-\alpha)\tilde{d}_{k-1} + \alpha d_k, \quad 0<\alpha<1;
  $$

- $t_k$: thời điểm tương ứng với $d_k$;  
- Ta xem cặp $(t_k, \tilde{d}_k)$ là điểm trong mặt phẳng “thời gian–delay”.

**Trendline estimation**:

- Xét $N$ điểm gần nhất $(t_i,\tilde{d}_i)$, $i = k-N+1,\dots,k$;  
- Tính slope $s_k$ bằng **linear regression**:

  $$
  s_k = \frac{ \sum_i (t_i - \bar{t})(\tilde{d}_i - \bar{d}) }{ \sum_i (t_i - \bar{t})^2 },
  $$

  với:

  $$
  \bar{t} = \frac{1}{N}\sum_i t_i,\quad
  \bar{d} = \frac{1}{N}\sum_i \tilde{d}_i.
  $$

Dựa trên $s_k$, GCC xác định trạng thái:

- Nếu $s_k > \theta_{\text{over}}$ (dương, vượt ngưỡng): **overuse**;  
- Nếu $s_k < \theta_{\text{under}}$ (âm, vượt về âm): **underuse**;  
- Nếu nằm trong khoảng: **normal**.

Ngưỡng $\theta_{\text{over}}, \theta_{\text{under}}$ có thể điều chỉnh theo noise level của delay.

Trực giác:

- Khi hàng đợi tại bottleneck tăng dần, delay gói tới sẽ tăng → trendline dốc lên;  
- Khi hàng đợi giảm, delay trendline dốc xuống.

Như vậy, GCC là một **delay-based CC** giống như Vegas/FAST, nhưng được tinh chỉnh cho môi trường real-time và sử dụng regression + Kalman filter trong một số phiên bản.

---

#### D.4.2.2. Bộ điều khiển tốc độ (rate controller) AIMD

Ký hiệu:

- $x_k$: ước lượng **bitrate mục tiêu** (target sending rate) tại thời điểm $k$ (bit/s);  
- $s_k \in \{\text{overuse}, \text{underuse}, \text{normal}\}$: trạng thái của delay estimator;  
- $G_{\text{up}}$: step tăng tuyến tính (bit/s) mỗi đơn vị thời gian khi underuse;  
- $\beta$: hệ số giảm multiplicative (0 < $\beta$ < 1).

Quy tắc cập nhật đơn giản:

- Nếu $s_k = \text{underuse}$ (queue trống, delay giảm):

  $$
  x_{k+1} = x_k + G_{\text{up}} \cdot \Delta t,
  $$

  trong đó $\Delta t$ là thời gian giữa hai cập nhật (ví dụ 100 ms).

- Nếu $s_k = \text{overuse}$:

  $$
  x_{k+1} = \beta x_k. 
  $$

- Nếu $s_k = \text{normal}$:

  $$
  x_{k+1} = x_k.
  $$

Đồng thời, GCC có một **loss-based component**:

- nếu tỷ lệ mất gói $p_k$ vượt ngưỡng (ví dụ 10%) dù delay chưa tăng nhiều, cũng giảm bitrate để tránh collapse.

Ở phía sender:

- Bộ điều khiển bitrate video (rate allocator) chọn $R_k \le x_k$:

  - có thể điều chỉnh:

    - QP của encoder,  
    - resolution,  
    - framerate,  
    - mức FEC/packetization,

  sao cho video output có bitrate thực tế gần $x_k$.

Do CC ở đây **điều khiển tốc độ của ứng dụng**, không phải cwnd như TCP, nên pipeline:

```text
[Video encoder] --(R_k bits/s)--> [RTP sender / packetizer] --UDP--> Network
                      ^
                      |
                 x_k (GCC)
```

GCC cũng có cơ chế **“time-since-last-overuse”** và hysteresis để tránh toggling trạng thái liên tục. Mô hình chi tiết sử dụng Kalman filter để ước lượng noise và điều chỉnh gain [D32, D33].

---

#### D.4.2.3. Liên hệ với QoE cho video real-time

Từ góc nhìn QoE:

* **Mục tiêu chính** của GCC:

  * giữ $T_{\text{queue}}$ thấp → giảm $RTT$ và **end-to-end latency** $T_{\text{E2E}}$;
  * tránh packet loss do queue tràn (congestion loss);
  * chấp nhận **biến động bitrate và chất lượng** $Q_k$.

So với TCP CUBIC/BBR:

* TCP cố gắng **tối đa hóa throughput** trong giới hạn fairness, ít quan tâm đến **latency tức thời** (trừ BBR cố hạn chế bufferbloat).
* GCC ưu tiên **latency + stability** hơn throughput – mục tiêu là **tương tác thời gian thực** chứ không phải throughput tối đa.

Trong mô hình QoE tổng quát (D.3):

$$\text{QoE} = \sum_{k=1}^{K} u(Q_k) - \alpha_T T_{\text{stall,total}} - \alpha_S \sum_{k=1}^{K-1} |Q_{k+1} - Q_k| - \alpha_L L_{\text{E2E}},$$

với streaming real-time:

* $T_{\text{stall,total}}$ hiếm khi được sử dụng (không “buffering” kiểu VoD), nhưng **tương đương** là panahon freeze / drop frame khi mạng xấu – GCC cố gắng giảm bằng cách giảm bitrate nhanh khi overuse.
* $L_{\text{E2E}}$ cực kỳ quan trọng (đặc biệt với video call, interactive streaming).
* $\alpha_L$ được coi **lớn** hơn so với VoD/OTT: vài trăm ms độ trễ thêm có thể làm QoE giảm mạnh, dù chất lượng $Q_k$ cao hơn.

Do đó:

* GCC thường **hi sinh** $Q_k$ (giảm bitrate, giảm framerate) để giữ $L_{\text{E2E}}$ nhỏ;
* Trong khi ABR trên TCP (DASH/HLS) thường **hi sinh** latency để tránh stall và duy trì chất lượng.

---

### Tài liệu tham khảo cho mục D.4 (định dạng IEEE)

[D27] J. Padhye, V. Firoiu, D. F. Towsley, and J. F. Kurose, “Modeling TCP throughput: A simple model and its empirical validation,” in *Proc. ACM SIGCOMM*, 1998, pp. 303–314.

[D28] S. Ha, I. Rhee, and L. Xu, “CUBIC: A new TCP-friendly high-speed TCP variant,” *ACM SIGOPS Oper. Syst. Rev.*, vol. 42, no. 5, pp. 64–74, Jul. 2008.

[D29] L. Xu *et al.*, “CUBIC for Fast and Long-Distance Networks,” RFC 8312, IETF, Feb. 2018; and draft-ietf-tcpm-rfc8312bis, IETF, 2021.

[D30] N. Cardwell, Y. Cheng, C. S. Gunn, S. H. Yeganeh, and V. Jacobson, “BBR: Congestion-based congestion control,” *Commun. ACM*, vol. 60, no. 2, pp. 58–66, Feb. 2017.

[D31] N. Cardwell *et al.*, “BBR Congestion Control: Fundamentals and Recent Updates,” Google Tech. Talk, 2023.

[D32] G. Carlucci, L. De Cicco, S. Holmer, and S. Mascolo, “Analysis and design of the Google congestion control for Web real-time communication (WebRTC),” *Computer Networks*, vol. 136, pp. 137–151, May 2018.

[D33] L. De Cicco, G. Carlucci, S. Mascolo, “Understanding the dynamic behaviour of the Google congestion control for RTCWeb,” in *Proc. IEEE Packet Video Workshop*, 2013.

[D34] H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobson, “RTP: A Transport Protocol for Real-Time Applications,” RFC 3550, IETF, Jul. 2003.

[D35] IETF, “TCP Congestion Control,” RFC 5681, IETF, Sep. 2009.

[D36] M. Hock, R. Bless, and M. Zitterbart, “Experimental evaluation of BBR congestion control,” in *Proc. IEEE ICNP*, 2017, pp. 1–10.

